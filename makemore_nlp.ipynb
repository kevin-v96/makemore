{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -----> e\n",
      "..e -----> m\n",
      ".em -----> m\n",
      "emm -----> a\n",
      "mma -----> .\n",
      "olivia\n",
      "... -----> o\n",
      "..o -----> l\n",
      ".ol -----> i\n",
      "oli -----> v\n",
      "liv -----> i\n",
      "ivi -----> a\n",
      "via -----> .\n",
      "ava\n",
      "... -----> a\n",
      "..a -----> v\n",
      ".av -----> a\n",
      "ava -----> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words[:3]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '----->', itos[ix])\n",
    "        context = context[1:] + [ix] #crop the last one and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [-1.1907, -0.0646]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [-1.1907, -0.0646],\n",
       "         [ 0.6483, -0.4550]],\n",
       "\n",
       "        [[-1.1907, -0.0646],\n",
       "         [ 0.6483, -0.4550],\n",
       "         [ 0.6483, -0.4550]],\n",
       "\n",
       "        [[ 0.6483, -0.4550],\n",
       "         [ 0.6483, -0.4550],\n",
       "         [-1.0422, -0.7855]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [-0.2528, -0.8239]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [-0.2528, -0.8239],\n",
       "         [-0.2224,  1.0154]],\n",
       "\n",
       "        [[-0.2528, -0.8239],\n",
       "         [-0.2224,  1.0154],\n",
       "         [-0.3499, -0.5857]],\n",
       "\n",
       "        [[-0.2224,  1.0154],\n",
       "         [-0.3499, -0.5857],\n",
       "         [ 0.2954, -0.5359]],\n",
       "\n",
       "        [[-0.3499, -0.5857],\n",
       "         [ 0.2954, -0.5359],\n",
       "         [-0.3499, -0.5857]],\n",
       "\n",
       "        [[ 0.2954, -0.5359],\n",
       "         [-0.3499, -0.5857],\n",
       "         [-1.0422, -0.7855]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [ 0.2989, -1.2410],\n",
       "         [-1.0422, -0.7855]],\n",
       "\n",
       "        [[ 0.2989, -1.2410],\n",
       "         [-1.0422, -0.7855],\n",
       "         [ 0.2954, -0.5359]],\n",
       "\n",
       "        [[-1.0422, -0.7855],\n",
       "         [ 0.2954, -0.5359],\n",
       "         [-1.0422, -0.7855]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.6745,  -7.4754,   5.6886,   2.5965,  -1.2465,  -6.5902,  -8.2576,\n",
       "          12.4431,   2.9072,   5.5546,  -8.8877,  -6.7650,   1.0405,  -3.1388,\n",
       "          14.1150,   3.0673,  -4.5765,   6.2176,  -6.1191,   6.0405,   0.5132,\n",
       "          -5.8822, -23.4996,  -6.3595, -10.3019,  -6.8006,  -0.0592],\n",
       "        [-10.2342,  -5.5114,  14.0960,  -1.1482,  -8.9194,  -7.4169, -12.3618,\n",
       "           6.6457,   4.8402,  -5.7327,  -1.2169, -10.3200,   0.6088,  -5.3768,\n",
       "          -3.5432,  10.3747,  -4.7056,   0.4381,  -6.3505,  -6.3350,  -1.2659,\n",
       "         -10.9178,  -4.9719,   2.7948,  -9.6452,  -2.1447,   0.2916],\n",
       "        [  1.9207,  -5.6206,  -0.8630,  -0.5479,  -3.4970, -16.7600,  -3.1978,\n",
       "           6.0159,   3.0366,  -3.2691,  -6.2295,  -2.1267,   3.3850,   8.8305,\n",
       "          -0.0874,  -8.0117,  -4.6370,   8.0025,  -8.0551,   6.7281,   3.9872,\n",
       "           2.7688, -15.2153,  -6.9070,   2.4371,  -8.2294,  -0.1412],\n",
       "        [ -1.3998,   6.1825,  -3.3062,   0.4939,  -2.0678,  -2.1343,   7.4536,\n",
       "           8.8321,  10.1507,   1.1171,   6.7359,  -5.4939,   2.0805,  -8.8272,\n",
       "          -2.4698,  -7.1406,   0.1909,  -3.5751,  -9.7995,  10.0268,   3.4173,\n",
       "          -6.4321,  -2.6696,   1.4032,  -5.5993, -14.6197,  -5.7830],\n",
       "        [-14.2954,  -6.5947,  11.3149,   0.2625, -11.9303,  -6.6485, -14.2130,\n",
       "           7.0556,   3.1625,  -8.0336,  -0.8376, -14.5392,   6.3553,  -1.5777,\n",
       "           1.9214,  14.3865,  -2.9568,   3.8092,  -4.4487,  -3.9922,  -3.5098,\n",
       "         -10.5545, -13.6470,  -3.5081, -12.4438,  -1.3966,   4.9125],\n",
       "        [ -6.6745,  -7.4754,   5.6886,   2.5965,  -1.2465,  -6.5902,  -8.2576,\n",
       "          12.4431,   2.9072,   5.5546,  -8.8877,  -6.7650,   1.0405,  -3.1388,\n",
       "          14.1150,   3.0673,  -4.5765,   6.2176,  -6.1191,   6.0405,   0.5132,\n",
       "          -5.8822, -23.4996,  -6.3595, -10.3019,  -6.8006,  -0.0592],\n",
       "        [ -9.1685,  -8.8123,   9.0794,   1.7949,  -5.3459,  -8.3018,  -8.4940,\n",
       "          13.7425,   4.2006,   1.0544,  -5.8068, -10.8171,   1.0828,  -6.2986,\n",
       "           7.8397,   7.2991,  -4.5794,   6.4637,  -4.9651,   3.3065,  -0.2768,\n",
       "          -7.6079, -21.6836,  -3.0141,  -9.8058,  -5.6453,  -1.0476],\n",
       "        [-10.8451,   3.9223,   9.0523,  -7.9098,  -7.9795, -12.7731,  -8.0852,\n",
       "           5.8797,   5.7512,  -4.7087,  -3.8041,  -5.0314,  -0.0638,   5.3558,\n",
       "           1.0633,  -1.2621,  -9.1961,  -1.2454,  -6.3602,   1.8663,  -0.1778,\n",
       "          -8.5389,  -7.6718,  -5.7177,  -6.2423,  -8.5993,  -3.4072],\n",
       "        [ -8.2347,  -0.8554,  -2.7518,  -7.9237,  -5.1050, -17.9306,  -4.1960,\n",
       "           5.3393,  -3.5734, -12.4472,   9.8305, -10.2940,   3.4714,   5.8176,\n",
       "          -6.2494,  -5.6458,  -5.2382,  14.6299,  -9.4555,   3.3210,   3.4661,\n",
       "          -4.1886,   2.1513,  -9.8009,  -5.7163, -14.2925,   1.1501],\n",
       "        [  3.3364,   8.0539, -11.6013,   9.5836,   0.2814,   5.5805,   5.2106,\n",
       "          -0.1019,  22.1249,  -1.9543, -10.5759,  -1.3672,  28.0690,  -5.9751,\n",
       "           8.3556,   0.8646,   9.6472,   0.0979,  -2.4945,   1.3133,  -8.6412,\n",
       "          -2.7172, -17.1783,  -5.3423,  -6.5165, -12.9334,   7.9985],\n",
       "        [ -6.5487,  -1.6728,   5.1475,  -0.1088,  -4.9563,  -1.8346,  -3.8080,\n",
       "          11.2652,   9.3050,  -4.2296,   6.5010, -13.9276,   6.0331,  -7.9809,\n",
       "          -0.6698,   1.6096,   0.7292,   2.3755, -10.5462,   5.2300,   0.7899,\n",
       "         -11.8571,  -9.7869,  -2.7609, -11.0251, -12.4806,  -2.9396],\n",
       "        [ -6.3307,  -5.3022,   8.9578,   0.2675,  -6.0841,  -4.4852,  -4.7063,\n",
       "           4.4242,   4.7372,  -5.4466,   2.4646,  -9.8304,  15.5651,  -1.3567,\n",
       "           1.5087,   7.3896,   2.8420,  10.3835,  -8.4036,  -1.9736,  -4.0219,\n",
       "          -2.4911, -11.7548,  -3.6018,  -9.1543, -11.7091,   1.6019],\n",
       "        [ -6.6745,  -7.4754,   5.6886,   2.5965,  -1.2465,  -6.5902,  -8.2576,\n",
       "          12.4431,   2.9072,   5.5546,  -8.8877,  -6.7650,   1.0405,  -3.1388,\n",
       "          14.1150,   3.0673,  -4.5765,   6.2176,  -6.1191,   6.0405,   0.5132,\n",
       "          -5.8822, -23.4996,  -6.3595, -10.3019,  -6.8006,  -0.0592],\n",
       "        [ -9.1026,  -8.7721,  12.5493,   2.1112,  -7.6437,  -7.2181, -12.4928,\n",
       "          11.5820,   4.9527,  -3.0187,   0.7539, -13.5889,   3.7301,  -7.2075,\n",
       "           1.1422,  12.4557,  -0.4796,   7.6545,  -5.9935,  -1.4590,  -2.6312,\n",
       "          -9.6812, -14.9228,   1.0749, -10.8119,  -3.7748,   0.6849],\n",
       "        [ -1.2123,  -9.3389,   5.7341,  -6.9398,  -0.7887, -10.1319,  -2.8597,\n",
       "           9.3808,   5.1039,   1.2142,  -6.4587,  -5.8393,   9.8860,   4.2031,\n",
       "           5.2480,  -4.4392,  -3.5204,   5.9270,  -8.8587,   7.7038,  -1.0548,\n",
       "          -2.9842, -21.3613,  -6.4100,  -1.3843, -14.1620,  -4.7040],\n",
       "        [ -4.3437,   2.1507,   6.8294,  -0.2084,  -0.3206,   5.4786,  -4.0958,\n",
       "           8.8566,   8.5902,  -9.3344,  15.2799, -16.0345,   0.6618, -11.1537,\n",
       "          -7.8056,  -3.2393,   4.2661,  -2.2681, -16.8520,   7.2462,   2.6733,\n",
       "          -7.8780,   4.0085,   1.2874,  -8.1090, -12.6836,  -1.3549]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.8717e-10, 3.5336e-10, 1.8419e-04, 8.3633e-06, 1.7923e-07, 8.5634e-10,\n",
       "         1.6163e-10, 1.5802e-01, 1.1412e-05, 1.6109e-04, 8.6073e-11, 7.1900e-10,\n",
       "         1.7646e-06, 2.7015e-08, 8.4103e-01, 1.3392e-05, 6.4151e-09, 3.1262e-04,\n",
       "         1.3717e-09, 2.6188e-04, 1.0415e-06, 1.7383e-09, 3.8816e-17, 1.0785e-09,\n",
       "         2.0925e-11, 6.9386e-10, 5.8753e-07],\n",
       "        [2.6476e-11, 2.9780e-09, 9.7571e-01, 2.3380e-07, 9.8592e-11, 4.4297e-10,\n",
       "         3.1537e-12, 5.6710e-04, 9.3234e-05, 2.3867e-09, 2.1827e-07, 2.4297e-11,\n",
       "         1.3548e-06, 3.4070e-09, 2.1315e-08, 2.3614e-02, 6.6663e-09, 1.1423e-06,\n",
       "         1.2867e-09, 1.3068e-09, 2.0784e-07, 1.3364e-11, 5.1074e-09, 1.2058e-05,\n",
       "         4.7710e-11, 8.6308e-08, 9.8660e-07],\n",
       "        [6.0864e-04, 3.2301e-07, 3.7621e-05, 5.1551e-05, 2.7006e-06, 4.6927e-12,\n",
       "         3.6426e-06, 3.6550e-02, 1.8578e-03, 3.3919e-06, 1.7570e-07, 1.0632e-05,\n",
       "         2.6321e-03, 6.0987e-01, 8.1708e-05, 2.9564e-08, 8.6376e-07, 2.6646e-01,\n",
       "         2.8309e-08, 7.4503e-02, 4.8063e-03, 1.4213e-03, 2.1994e-11, 8.9232e-08,\n",
       "         1.0201e-03, 2.3779e-08, 7.7424e-05],\n",
       "        [4.2389e-06, 8.3214e-03, 6.2993e-07, 2.8162e-05, 2.1733e-06, 2.0336e-06,\n",
       "         2.9664e-02, 1.1773e-01, 4.4013e-01, 5.2516e-05, 1.4471e-02, 7.0667e-08,\n",
       "         1.3763e-04, 2.5208e-09, 1.4539e-06, 1.3616e-08, 2.0801e-05, 4.8141e-07,\n",
       "         9.5342e-10, 3.8884e-01, 5.2393e-04, 2.7653e-08, 1.1906e-06, 6.9916e-05,\n",
       "         6.3595e-08, 7.6894e-12, 5.2925e-08],\n",
       "        [3.3377e-13, 7.3763e-10, 4.4249e-02, 7.0128e-07, 3.5533e-12, 6.9898e-10,\n",
       "         3.6245e-13, 6.2531e-04, 1.2745e-05, 1.7496e-10, 2.3340e-07, 2.6157e-13,\n",
       "         3.1042e-04, 1.1135e-07, 3.6841e-06, 9.5470e-01, 2.8039e-08, 2.4332e-05,\n",
       "         6.3071e-09, 9.9560e-09, 1.6129e-08, 1.4064e-11, 6.3837e-13, 1.6156e-08,\n",
       "         2.1262e-12, 1.3346e-07, 7.3342e-05],\n",
       "        [7.8717e-10, 3.5336e-10, 1.8419e-04, 8.3633e-06, 1.7923e-07, 8.5634e-10,\n",
       "         1.6163e-10, 1.5802e-01, 1.1412e-05, 1.6109e-04, 8.6073e-11, 7.1900e-10,\n",
       "         1.7646e-06, 2.7015e-08, 8.4103e-01, 1.3392e-05, 6.4151e-09, 3.1262e-04,\n",
       "         1.3717e-09, 2.6188e-04, 1.0415e-06, 1.7383e-09, 3.8816e-17, 1.0785e-09,\n",
       "         2.0925e-11, 6.9386e-10, 5.8753e-07],\n",
       "        [1.1057e-10, 1.5788e-10, 9.3018e-03, 6.3820e-06, 5.0553e-09, 2.6304e-10,\n",
       "         2.1704e-10, 9.8564e-01, 7.0747e-05, 3.0433e-06, 3.1884e-09, 2.1264e-11,\n",
       "         3.1312e-06, 1.9498e-09, 2.6926e-03, 1.5682e-03, 1.0879e-08, 6.8009e-04,\n",
       "         7.3978e-09, 2.8936e-05, 8.0397e-07, 5.2646e-10, 4.0584e-16, 5.2050e-08,\n",
       "         5.8454e-11, 3.7473e-09, 3.7195e-07],\n",
       "        [2.0560e-09, 5.3258e-03, 9.0022e-01, 3.8708e-08, 3.6099e-08, 2.9900e-10,\n",
       "         3.2481e-08, 3.7711e-02, 3.3166e-02, 9.5065e-07, 2.3489e-06, 6.8842e-07,\n",
       "         9.8914e-05, 2.2333e-02, 3.0530e-04, 2.9843e-05, 1.0695e-08, 3.0346e-05,\n",
       "         1.8230e-07, 6.8152e-04, 8.8253e-05, 2.0632e-08, 4.9109e-08, 3.4658e-07,\n",
       "         2.0511e-07, 1.9424e-08, 3.4932e-06],\n",
       "        [1.1651e-10, 1.8670e-07, 2.8026e-08, 1.5900e-10, 2.6642e-09, 7.1695e-15,\n",
       "         6.6119e-09, 9.1506e-05, 1.2324e-08, 1.7254e-12, 8.1653e-03, 1.4860e-11,\n",
       "         1.4134e-05, 1.4764e-04, 8.4835e-10, 1.5514e-09, 2.3319e-09, 9.9155e-01,\n",
       "         3.4369e-11, 1.2160e-05, 1.4058e-05, 6.6612e-09, 3.7751e-06, 2.4331e-11,\n",
       "         1.4457e-09, 2.7258e-13, 1.3872e-06],\n",
       "        [1.8098e-11, 2.0249e-09, 5.8918e-18, 9.3491e-09, 8.5284e-13, 1.7069e-10,\n",
       "         1.1792e-10, 5.8129e-13, 2.6144e-03, 9.1179e-14, 1.6429e-17, 1.6401e-13,\n",
       "         9.9739e-01, 1.6356e-15, 2.7381e-09, 1.5280e-12, 9.9627e-09, 7.0987e-13,\n",
       "         5.3124e-14, 2.3933e-12, 1.1371e-16, 4.2517e-14, 2.2295e-20, 3.0796e-15,\n",
       "         9.5184e-16, 1.5551e-18, 1.9157e-09],\n",
       "        [1.5819e-08, 2.0739e-06, 1.9003e-03, 9.9086e-06, 7.7767e-08, 1.7640e-06,\n",
       "         2.4517e-07, 8.6239e-01, 1.2144e-01, 1.6084e-07, 7.3558e-03, 9.8760e-12,\n",
       "         4.6069e-03, 3.7777e-09, 5.6544e-06, 5.5249e-05, 2.2906e-05, 1.1884e-04,\n",
       "         2.9049e-10, 2.0636e-03, 2.4339e-05, 7.8305e-11, 6.2069e-10, 6.9862e-07,\n",
       "         1.7995e-10, 4.1979e-11, 5.8428e-07],\n",
       "        [3.0734e-10, 8.5963e-10, 1.3407e-03, 2.2553e-07, 3.9330e-10, 1.9459e-09,\n",
       "         1.5600e-09, 1.4402e-05, 1.9695e-05, 7.4402e-10, 2.0295e-06, 9.2839e-12,\n",
       "         9.9276e-01, 4.4446e-08, 7.8030e-07, 2.7943e-04, 2.9600e-06, 5.5789e-03,\n",
       "         3.8671e-11, 2.3984e-08, 3.0928e-09, 1.4295e-08, 1.3551e-12, 4.7077e-09,\n",
       "         1.8255e-11, 1.4185e-12, 8.5653e-07],\n",
       "        [7.8717e-10, 3.5336e-10, 1.8419e-04, 8.3633e-06, 1.7923e-07, 8.5634e-10,\n",
       "         1.6163e-10, 1.5802e-01, 1.1412e-05, 1.6109e-04, 8.6073e-11, 7.1900e-10,\n",
       "         1.7646e-06, 2.7015e-08, 8.4103e-01, 1.3392e-05, 6.4151e-09, 3.1262e-04,\n",
       "         1.3717e-09, 2.6188e-04, 1.0415e-06, 1.7383e-09, 3.8816e-17, 1.0785e-09,\n",
       "         2.0925e-11, 6.9386e-10, 5.8753e-07],\n",
       "        [1.7186e-10, 2.3917e-10, 4.3498e-01, 1.2742e-05, 7.3916e-10, 1.1313e-09,\n",
       "         5.7919e-12, 1.6533e-01, 2.1842e-04, 7.5397e-08, 3.2793e-06, 1.9355e-12,\n",
       "         6.4319e-05, 1.1434e-09, 4.8351e-06, 3.9612e-01, 9.5519e-07, 3.2560e-03,\n",
       "         3.8499e-09, 3.5872e-07, 1.1108e-07, 9.6354e-11, 5.0988e-13, 4.5207e-06,\n",
       "         3.1104e-11, 3.5401e-08, 3.0607e-06],\n",
       "        [8.5394e-06, 2.5240e-09, 8.8757e-03, 2.7797e-08, 1.3044e-05, 1.1421e-09,\n",
       "         1.6442e-06, 3.4036e-01, 4.7264e-03, 9.6662e-05, 4.4972e-08, 8.3548e-08,\n",
       "         5.6413e-01, 1.9200e-03, 5.4591e-03, 3.3884e-07, 8.4922e-07, 1.0765e-02,\n",
       "         4.0797e-09, 6.3629e-02, 9.9958e-06, 1.4518e-06, 1.5165e-14, 4.7218e-08,\n",
       "         7.1902e-06, 2.0298e-11, 2.6002e-07],\n",
       "        [2.9926e-09, 1.9794e-06, 2.1303e-04, 1.8706e-07, 1.6721e-07, 5.5182e-05,\n",
       "         3.8344e-09, 1.6176e-03, 1.2393e-03, 2.0352e-11, 9.9652e-01, 2.5049e-14,\n",
       "         4.4659e-07, 3.3000e-12, 9.3875e-11, 9.0294e-09, 1.6415e-05, 2.3848e-08,\n",
       "         1.1061e-14, 3.2320e-04, 3.3381e-06, 8.7318e-11, 1.2687e-05, 8.3483e-07,\n",
       "         6.9307e-11, 7.1464e-13, 5.9438e-08]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.5634e-10, 3.4070e-09, 6.0987e-01, 8.3214e-03, 3.3377e-13, 1.3392e-05,\n",
       "        3.1312e-06, 9.5065e-07, 3.7751e-06, 9.1179e-14, 2.0739e-06, 3.0734e-10,\n",
       "        3.5336e-10, 5.0988e-13, 2.5240e-09, 2.9926e-09])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilities of the actual Y\n",
    "probs[torch.arange(16), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4465)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(16), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring everything together before we train our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of parameters\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.162921905517578"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] #(16, 3, 2) 16 letters, 3 at a time, 2 dimensional representation in the lookup table\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) #(16, 100)\n",
    "logits = h @ W2 + b2 #(16, 27)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "loss = -probs[torch.arange(16), Y].log().mean()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
